\chapter{Training Methodology and Models}
\label{ch8_training_and_models}

In questo capitolo vengono presentate in dettaglio le metodologie di addestramento adottate e le architetture dei modelli di Reinforcement Learning (RL) implementati per la navigazione del rover. L'obiettivo sperimentale di questo lavoro è duplice: in primo luogo, validare l'efficacia di una pipeline di simulazione massiva basata su Isaac Lab per l'apprendimento di task di locomozione complessa; in secondo luogo, dimostrare formalmente e sperimentalmente l'ipotesi che l'introduzione di una componente di memoria — sia essa esplicita (Frame Stacking), ricorrente (RNN) o predittiva (World Models) — costituisca una condizione necessaria e sufficiente per garantire la sicurezza della navigazione in ambienti parzialmente osservabili (POMDP) come quelli lunari.

La campagna sperimentale è stata concepita secondo un approccio incrementale, partendo da baseline reattive semplici per arrivare ad architetture cognitive complesse. Nello specifico, sono stati analizzati:
\begin{enumerate}
    \item \textbf{Modelli Reattivi (Memoryless)}: PPO e SAC basati su singole osservazioni istantanee, per stabilire una baseline di prestazione minima.
    \item \textbf{Modelli con Memoria Temporale}: Varianti con \emph{Frame Stacking} ($N=16$) per ricostruire implicitamente le derivate dello stato (velocità, accelerazione).
    \item \textbf{Modelli Ricorrenti}: Recurrent PPO con celle LSTM, per gestire la parziale osservabilità su orizzonti temporali indefiniti.
    \item \textbf{World Models}: TD-MPC2, per abilitare la pianificazione nel dominio latente e migliorare la \emph{sample efficiency}.
\end{enumerate}

\section{Training Pipeline Architecture}
\label{sec:training_pipeline}

L'infrastruttura di addestramento rappresenta il ponte tra la simulazione fisica e l'ottimizzazione della policy. È stata sviluppata integrando il framework \textbf{Isaac Lab} con la libreria \textbf{Stable-Baselines3} (SB3) per gli algoritmi model-free standard, e con un framework custom basato su PyTorch per TD-MPC2.
Il punto di ingresso principale è lo script \texttt{isaaclab\_train.py}, che orchestra l'intero ciclo di vita dell'esperimento.

\subsection{Isaac Lab: Parallelizzazione Massiva e Vectorized Environments}
La scelta di Isaac Lab è motivata dalla necessità di raccogliere enormi quantità di dati in tempi ridotti. A differenza dei simulatori tradizionali (e.g., Gazebo) che richiedono un processo OS per ogni robot, Isaac Lab sfrutta l'API di PhysX 5 per simulare $N$ ambienti indipendenti all'interno di un singolo contesto CUDA.

L'ambiente, definito nella classe \texttt{RoverNavigationEnv}, restituisce le osservazioni sotto forma di tensori PyTorch residenti in VRAM:
\[
\mathcal{O}_t \in \mathbb{R}^{N_{envs} \times D_{obs}}
\]
dove $N_{envs}$ è il numero di ambienti paralleli (configurato a 64 o 128) e $D_{obs}$ la dimensione dello spazio delle osservazioni. Questo elimina l'overhead di trasferimento memoria CPU-GPU (\emph{device-to-host}), permettendo throughput nell'ordine di $10^4$ steps/secondo.

\subsubsection{Curriculum Learning Dinamico}
Per evitare che l'agente converga su minimi locali sub-ottimali (e.g., ruotare su sé stesso per evitare collisioni), è stato implementato un meccanismo di Curriculum Learning automatico. La difficoltà dell'ambiente, parametrizzata dalla densità di ostacoli $\rho_{rocks}$ e delle pendenze $\theta_{slope}$, evolve secondo una funzione a gradino basata sul tasso di successo medio $\bar{S}$:

\[
\text{Level}_{t+1} = 
\begin{cases} 
\text{Level}_t + 1 & \text{se } \bar{S}_t > 0.8 \\
\text{Level}_t & \text{altrimenti}
\end{cases}
\]
I parametri iniziali prevedono un ambiente sparso ($\rho_{rocks} \approx 0.1 \, m^{-2}$), scalando fino alla densità target ($\rho_{rocks} \approx 0.5 \, m^{-2}$) tipica dei campi di massi lunari.

\subsection{Logging, Checkpointing e Callbacks}
Il monitoraggio del training è gestito tramite un sistema di callback modulari integrate in SB3, essenziali per l'analisi scientifica dei risultati:
\begin{itemize}
    \item \textbf{CheckpointCallback}: Serializza lo stato completo del modello (pesi della rete, stato dell'ottimizzatore, buffer di normalizzazione) ogni $50.000$ step. Questo permette non solo il ripristino in caso di crash, ma anche l'analisi a posteriori dell'evoluzione della policy (`save\_freq` configurabile).
    \item \textbf{CsvStepLoggerCallback}: Registra ad ogni step di rollout le metriche scalari fondamentali: loss di policy ($L_\pi$), loss di valore ($L_V$), entropia ($H$), reward media e lunghezza degli episodi.
    \item \textbf{ReplayBufferLoggerCallback}: Specifica per algoritmi off-policy (SAC, TD-MPC2), estrae periodicamente un sottoinsieme del buffer di esperienza per analizzare la distribuzione delle transizioni visitate e verificare problemi di \emph{catastrophic forgetting}.
\end{itemize}

\section{Proximal Policy Optimization (PPO)}
\label{sec:ppo_model}

Proximal Policy Optimization (PPO) è stato selezionato come algoritmo on-policy di riferimento per la sua stabilità numerica e la capacità di gestire spazi di azione continui. PPO appartiene alla famiglia dei metodi Actor-Critic e ottimizza una funzione obiettivo "surrogata" che impedisce aggiornamenti troppo aggressivi della policy, garantendo un miglioramento monotonico delle prestazioni.

\subsection{Formulazione Matematica}
Sia $\pi_\theta(a|s)$ la policy stocastica parametrizzata da $\theta$. L'obiettivo di PPO è massimizzare la funzione $L^{CLIP}(\theta)$:

\[
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]
\]

Dove:
\begin{itemize}
    \item $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ è il rapporto di probabilità tra la nuova e la vecchia policy.
    \item $\hat{A}_t$ è la stima del vantaggio (\emph{Advantage}) calcolata tramite GAE (Generalized Advantage Estimation).
    \item $\epsilon$ è un iperparametro che definisce la regione di fiducia (tipicamente $0.2$), limitando la divergenza tra $\pi_\theta$ e $\pi_{\theta_{old}}$.
\end{itemize}

L'ottimizzazione avviene tramite discesa del gradiente stocastico (Adam) su mini-batch di esperienza raccolta. La loss totale minimizzata durante il training include anche termini per la funzione valore e l'entropia:

\[
L_t(\theta) = -L^{CLIP}_t(\theta) + c_1 L^{VF}_t(\theta) - c_2 S[\pi_\theta](s_t)
\]
dove $L^{VF}$ è l'errore quadratico medio della funzione valore $V_\phi(s)$ e $S$ è l'entropia della policy, che incoraggia l'esplorazione.

\subsection{Implementazione e Architettura}
In questo lavoro, la policy è implementata come un Perceptron Multistrato (MLP). La configurazione scelta prevede:
\begin{itemize}
    \item \textbf{Feature Extractor}: Dato lo stato normalizzato $s_t$, non viene usata una CNN (poiché l'input percettivo è già processato dalla pipeline geometrica), ma un layer lineare diretto.
    \item \textbf{Network Body}: Tre layer nascosti densi da 256 neuroni ciascuno (\texttt{net\_arch=[256, 256, 256]}) con funzioni di attivazione \emph{Tanh}. Questa profondità è stata scelta empiricamente per catturare le non linearità della dinamica di interazione ruota-suolo.
    \item \textbf{Output Heads}: 
    \begin{itemize}
        \item \emph{Actor}: Restituisce la media $\mu(s)$ di una distribuzione Gaussiana multivariata diagonale. La deviazione standard $\sigma$ è un parametro apprendibile ma indipendente dallo stato.
        \item \emph{Critic}: Restituisce lo scalare $V(s)$.
    \end{itemize}
\end{itemize}

\subsection{Analisi delle Configurazioni: NoMem vs FrameStack}
Per isolare scientificamente l'impatto della memoria, PPO è stato addestrato in due modalità:
\begin{enumerate}
    \item \textbf{PPO-NoMem}: L'input è $s_t = [scan_t, v_t, \omega_t, g_t]$. Questo configura il problema come un MDP standard. Tuttavia, in presenza di slittamento o occlusione, lo stato $s_t$ non è Markoviano.
    \item \textbf{PPO-FrameStack}: L'input è una concatenazione degli ultimi $k=16$ stati: $S_t = [s_{t}, s_{t-1}, \dots, s_{t-k+1}]$. Matematicamente, questo trasforma lo spazio delle osservazioni $\mathcal{O} \subseteq \mathbb{R}^D$ in $\mathcal{O}^k \subseteq \mathbb{R}^{D \times k}$. Questo permette alla rete MLP di apprendere filtri temporali simili a convoluzioni 1D, approssimando termini derivativi come l'accelerazione $\dot{v} \approx (v_t - v_{t-1})/\Delta t$.
\end{enumerate}

\section{Soft Actor-Critic (SAC)}
\label{sec:sac_model}

Mentre PPO rappresenta lo stato dell'arte per gli algoritmi on-policy, \textbf{Soft Actor-Critic (SAC)} è stato selezionato come rappresentante della famiglia \emph{off-policy} per valutare i benefici in termini di \emph{sample efficiency}. In un contesto di simulazione massiva, dove il costo computazionale è dominato dalla fisica, un algoritmo capace di riutilizzare l'esperienza passata (tramite Replay Buffer) può teoricamente convergere con un numero inferiore di step ambientali.

\subsection{Formulazione Matematica: Maximum Entropy RL}
A differenza degli algoritmi RL standard che massimizzano solo la somma cumulativa delle ricompense attese, SAC ottimizza un obiettivo di \emph{Maximum Entropy Reinforcement Learning}. La funzione obiettivo $J(\pi)$ è aumentata con un termine di entropia:

\[
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) \right]
\]

dove $\mathcal{H}(\pi(\cdot|s_t)) = -\mathbb{E}_{a \sim \pi} [\log \pi(a|s_t)]$ rappresenta l'entropia della policy e $\alpha$ è il coefficiente di temperatura che bilancia il trade-off tra sfruttamento (massimizzazione della reward) ed esplorazione (massimizzazione dell'entropia).
Questa formulazione matematica ha due vantaggi cruciali per la navigazione del rover:
\begin{enumerate}
    \item \textbf{Esplorazione Robusta}: L'agente è incentivato a esplorare azioni diverse se queste portano a ricompense simili, prevenendo la convergenza prematura su strategie sub-ottime (e.g., rimanere fermi per evitare collisioni).
    \item \textbf{Robustezza al Rumore}: La policy appresa risulta più robusta a perturbazioni nella dinamica, poiché impara un ventaglio di azioni valide piuttosto che una singola traiettoria deterministica.
\end{enumerate}

\subsection{Architettura Actor-Critic e Soft Updates}
L'implementazione utilizzata in questo lavoro segue l'architettura standard Actor-Critic con reti neurali separate:
\begin{itemize}
    \item \textbf{Soft Q-Function (Critic)}: Due reti $Q_{\phi_1}, Q_{\phi_2}$ vengono addestrate per minimizzare l'errore quadratico di Bellman modificato (soft Bellman residual):
    \[
    J_Q(\phi) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_\phi(s,a) - (r + \gamma (1-d) V_{\bar{\phi}}(s')) \right)^2 \right]
    \]
    dove il valore target $V$ include il termine di entropia. L'uso di due critici (\emph{Clipped Double Q-Learning}) riduce la sovrastima sistematica del valore tipica del Q-Learning.
    
    \item \textbf{Policy Network (Actor)}: La policy $\pi_\theta$ viene aggiornata tramite il \emph{reparameterization trick}. L'azione viene campionata come $a_\theta(s, \xi) = \tanh(\mu_\theta(s) + \sigma_\theta(s) \odot \xi)$, dove $\xi \sim \mathcal{N}(0, I)$. Questo rende l'operazione di campionamento differenziabile, permettendo al gradiente di fluire dal critico all'attore.
\end{itemize}

Anche per SAC, le reti sono configurate come MLP con tre layer nascosti da 256 unità (\texttt{net\_arch=[256, 256, 256]}). I target network vengono aggiornati utilizzando la media mobile esponenziale (\emph{Polyak averaging}) con un coefficiente $\tau \ll 1$ (Soft Update), garantendo stabilità durante il training.

\section{Recurrent PPO (LSTM)}
\label{sec:recurrent_ppo}

Le architetture basate su Frame Stacking ($N=16$), pur migliorando le prestazioni rispetto ai modelli memoryless, soffrono di un limite strutturale: l'orizzonte temporale è fisso. Se un ostacolo critico è stato osservato 20 frame fa e poi occluso, l'agente "dimentica" la sua esistenza. Per superare questo limite e gestire formalmente la navigazione come un problema POMDP (\emph{Partially Observable Markov Decision Process}), è stato implementato **Recurrent PPO**.

\subsection{Dinamica dello Stato Nascosto}
In R-PPO, la policy $\pi_\theta(a_t | o_t, h_{t-1})$ non dipende solo dall'osservazione corrente $o_t$, ma anche da uno stato nascosto $h_{t-1}$ che funge da memoria a lungo termine. L'aggiornamento dello stato avviene tramite una cella LSTM (\emph{Long Short-Term Memory}):

\[
h_t, c_t = \text{LSTM}(f(o_t), h_{t-1}, c_{t-1}; \theta_{lstm})
\]

Dove $f(o_t)$ è l'embedding estratto dai layer lineari iniziali e $(h_t, c_t)$ sono rispettivamente l'hidden state e il cell state.
Matematicamente, questo permette all'agente di approssimare la funzione di credenza (\emph{belief state}) $b_t(s) = P(s_t | o_{1:t}, a_{1:t-1})$ utilizzando un vettore di dimensione fissa, senza dover mantenere l'intera storia delle osservazioni.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{imgs/08_training_and_models/lstm.png}
    \caption{Schema concettuale della architettura Long Short-Therm Memory, che elimina la costrizione della memoria delle precedenti architetture da un numero definito di Frame N ad un orizzonte teoricamente illimitato}
    \label{fig:teapac-architecture}
\end{figure}


\subsection{Configurazione e Training: BPTT}
L'architettura di rete specifica prevede:
\begin{itemize}
    \item \textbf{Feature Extractor}: Un MLP che riduce la dimensionalità dell'osservazione.
    \item \textbf{LSTM Core}: Un singolo layer LSTM con 256 unità nascoste (\texttt{lstm\_hidden\_size=256}, \texttt{n\_lstm\_layers=1}).
    \item \textbf{Heads}: Layer separati per policy e value function che prendono in input l'output dell'LSTM.
\end{itemize}

L'addestramento di reti ricorrenti in RL introduce la complessità della \emph{Backpropagation Through Time} (BPTT). Per rendere il training computazionalmente trattabile su sequenze lunghe milioni di step, si utilizza la tecnica della **Truncated BPTT**.
Durante la fase di raccolta dati (rollout), le traiettorie vengono salvate insieme agli stati nascosti. Durante l'ottimizzazione, i gradienti vengono propagati indietro nel tempo solo per un numero limitato di step (finestra di unroll), impedendo l'esplosione o la scomparsa del gradiente, ma permettendo comunque di catturare dipendenze temporali significative (e.g., la presenza di una roccia vista secondi prima).

Questa architettura è specificamente progettata per risolvere situazioni di "cecità temporanea", come quando il rover esegue una rotazione sul posto ($rotation-in-place$): durante la manovra, gli ostacoli frontali escono dal campo visivo, ma l'LSTM mantiene la loro rappresentazione latente in $h_t$, inibendo l'avanzamento prematuro.


% --------------------------------------------------------------------------------------
\section{World-model TD-MPC2}
\label{sec:tdmpc2_model}

L'ultimo e più avanzato modello integrato in questo lavoro è **TD-MPC2** (\emph{Temporal Difference Model Predictive Control 2}), un algoritmo che rappresenta lo stato dell'arte nel \emph{model-based reinforcement learning}, recentemente introdotto da Hansen et al. (2024).
Rispetto agli approcci model-free discussi in precedenza (PPO, SAC), TD-MPC2 introduce un cambio di paradigma fondamentale: invece di apprendere una mappatura statica diretta tra osservazioni e azioni, l'agente apprende un modello implicito del mondo (\emph{World Model}) e utilizza tecniche di controllo predittivo nel dominio latente per pianificare le azioni ottimali.

Questa architettura è stata selezionata per la sua robustezza agli iperparametri e per la capacità di scalare efficacemente su task di controllo continuo complessi, gestendo dataset misti e multi-task senza richiedere modifiche strutturali \cite{tdmpc2}.

\subsection{Task-Oriented Latent Dynamics (TOLD)}
A differenza dei modelli del mondo generativi (e.g., Dreamer), che cercano di ricostruire le osservazioni visive future (pixel-reconstruction) — un compito computazionalmente oneroso e spesso superfluo per il controllo — TD-MPC2 adotta l'architettura **TOLD**. L'obiettivo è apprendere una rappresentazione latente compatta $z$ che codifichi esclusivamente le informazioni necessarie per predire le ricompense future e i valori degli stati, ignorando i dettagli visivi irrilevanti.

Il modello appreso è composto da cinque componenti principali, implementati come Perceptron Multistrato (MLP) con normalizzazione \emph{LayerNorm} e attivazioni \emph{Mish} \cite{tdmpc2}:

\begin{enumerate}
    \item \textbf{Encoder} ($h_\theta$): Mappa l'osservazione sensoriale corrente $s$ in una rappresentazione latente $z_0$:
    \[ z_t = h_\theta(s_t) \]
    
    \item \textbf{Latent Dynamics} ($d_\theta$): Predice lo stato latente futuro $z_{t+1}$ in funzione dello stato corrente $z_t$ e dell'azione $a_t$, senza mai decodificare nello spazio delle immagini:
    \[ z_{t+1} = d_\theta(z_t, a_t) \]
    
    \item \textbf{Reward Predictor} ($R_\theta$): Stima la ricompensa immediata associata alla transizione latente:
    \[ \hat{r}_t = R_\theta(z_t, a_t) \]
    
    \item \textbf{Value Function} ($Q_\theta$): Stima il valore terminale (return cumulativo scontato) dello stato latente, permettendo al planner di ragionare oltre l'orizzonte di simulazione:
    \[ \hat{q}_t = Q_\theta(z_t, a_t) \]
    
    \item \textbf{Policy Prior} ($\pi_\theta$): Una policy appresa che propone azioni potenzialmente ottimali per guidare il campionamento del planner e ridurre lo spazio di ricerca:
    \[ \hat{a}_t \sim \pi_\theta(z_t) \]
\end{enumerate}

Tutti i componenti sono condizionati, nel caso multi-task, da un embedding del task apprendibile, che permette al modello di generalizzare tra diverse dinamiche ambientali \cite{tdmpc2}.

\subsection{Latent Space Regularization: SimNorm}
Una delle innovazioni critiche di TD-MPC2, fondamentale per garantire la stabilità del training su Isaac Lab ed evitare il collasso delle rappresentazioni, è l'introduzione della \textbf{Simplicial Normalization (SimNorm)}.
Poiché il modello non è vincolato da una loss di ricostruzione visiva, lo spazio latente potrebbe divergere o diventare arbitrariamente grande.

SimNorm proietta il vettore latente $z$ in un insieme di $L$ simplessi di dimensione $V$ utilizzando un'operazione Softmax. Formalmente, dato un sottovettore $z_{sub}$, la normalizzazione è definita come:
\[
g_i = \frac{e^{z_{i \cdot V} / \tau}}{\sum_{j=1}^{V} e^{z_{j} / \tau}}, \quad z_{norm} = [g_1, \dots, g_L]
\]
Questa tecnica biasima la rappresentazione verso la sparsità senza imporre vincoli discreti rigidi (come in VQ-VAE), mantenendo la differenziabilità necessaria per la backpropagation attraverso il tempo e prevenendo l'esplosione dei gradienti \cite{tdmpc2}.

\subsection{Training Objective e Discrete Regression}
L'addestramento del World Model avviene ottimizzando congiuntamente tutte le componenti per minimizzare una funzione di costo composita su traiettorie campionate dal Replay Buffer. Un dettaglio implementativo cruciale per la robustezza è l'uso della \textbf{regolarizzazione discreta} per la predizione di reward e value.

Invece di utilizzare l'errore quadratico medio (MSE), che è sensibile alla scala delle ricompense (variabile tra i diversi scenari di training), TD-MPC2 discretizza lo spazio dei valori e tratta la predizione come un problema di classificazione multiclasse (cross-entropy) nello spazio log-trasformato \cite{tdmpc2}. Questo rende l'algoritmo intrinsecamente robusto a diverse scale di magnitudine della reward function definita nel Capitolo 6.

\subsection{Inference: Model Predictive Path Integral (MPPI)}
Durante l'esecuzione, l'agente non agisce direttamente tramite la policy prior $\pi_\theta(z)$. Invece, utilizza il modello appreso per pianificare tramite \textbf{MPPI} (\emph{Model Predictive Path Integral}), un metodo di controllo predittivo basato sul campionamento.

L'algoritmo di pianificazione segue questi passi per ogni step di controllo:
\begin{enumerate}
    \item \textbf{Sampling}: Vengono generate $K=512$ sequenze di azioni candidate lungo un orizzonte $H=6$ da una distribuzione gaussiana $\mathcal{N}(\mu, \sigma^2)$, inizializzata utilizzando le predizioni della Policy Prior.
    \item \textbf{Latent Rollout}: Per ogni sequenza, il modello dinamico predice l'evoluzione degli stati latenti $z_{t:t+H}$ e le relative ricompense.
    \item \textbf{Evaluation}: Il punteggio di ogni traiettoria è calcolato sommando le ricompense previste e il valore terminale stimato al passo $H$:
    \[ G(\tau) = \sum_{k=0}^{H-1} \gamma^k \hat{r}_{t+k} + \gamma^H Q_\theta(z_{t+H}, a_{t+H}) \]
    L'uso del termine terminale $Q_\theta$ è fondamentale: permette di pianificare su un orizzonte infinito pur simulando solo $H$ passi ("Bootstrapping") \cite{tdmpc2}.
    \item \textbf{Update}: La media $\mu$ della distribuzione di campionamento viene aggiornata verso le traiettorie a punteggio più alto, e la prima azione ottimizzata viene inviata ai motori.
\end{enumerate}

Questo approccio ibrido combina la flessibilità del RL (la Value Function apprende strategie a lungo termine) con la robustezza del controllo locale (MPPI corregge errori della policy), rendendo TD-MPC2 ideale per la navigazione su terreni incerti.




%--------------------------------------------------------------------------------------
\section{Hyperparameter Selection and Tuning}
\label{sec:hyperparameters}

La convergenza degli algoritmi di Reinforcement Learning è notoriamente sensibile alla scelta degli iperparametri. In questo lavoro, la strategia di tuning non ha mirato esclusivamente alla massimizzazione del \emph{return} in simulazione, ma ha dovuto soddisfare un vincolo di ottimizzazione vincolata: massimizzare le prestazioni della policy soggette ai limiti di latenza per l'inferenza in tempo reale sul processore del rover (Raspberry Pi 5).

\subsection{Configurazione Comparativa}
Per garantire un confronto equo (\emph{apples-to-apples}) tra le diverse architetture, sono stati mantenuti costanti i parametri comuni (e.g., discount factor, struttura del reward), variando solo quelli specifici dell'algoritmo.
La Tabella~\ref{tab:hyperparams} riassume la configurazione finale adottata per le tre classi di modelli.

\begin{table}[ht]
    \centering
    \caption{Iperparametri finali utilizzati per l'addestramento dei modelli. I valori sono stati selezionati per bilanciare stabilità di apprendimento ed efficienza computazionale.}
    \label{tab:hyperparams}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Parametro} & \textbf{PPO / R-PPO} & \textbf{SAC} & \textbf{TD-MPC2} \\
        \midrule
        \multicolumn{4}{l}{\textit{Ottimizzazione}} \\
        Optimizer & Adam & Adam & Adam \\
        Learning Rate ($\alpha$) & $3 \cdot 10^{-4}$ & $3 \cdot 10^{-4}$ & $1 \cdot 10^{-3}$ \\
        Batch Size & $128$ & $256$ & $256$ \\
        Discount Factor ($\gamma$) & $0.99$ & $0.99$ & $0.99$ \\
        \midrule
        \multicolumn{4}{l}{\textit{Architettura Rete}} \\
        Hidden Layers & $[256, 256, 256]$ & $[256, 256, 256]$ & $[512, 512]$ (MLP) \\
        Activation & Tanh & ReLU & Mish \\
        Latent Dim ($z$) & N/A & N/A & $512$ \\
        Sequence Len & $16$ (FrameStack) & $1$ & $6$ (Planning Horizon) \\
        \midrule
        \multicolumn{4}{l}{\textit{Specifici dell'Algoritmo}} \\
        Entropy Coef & $0.01$ & Automatic ($\alpha$) & N/A \\
        Clip Range ($\epsilon$) & $0.2$ & N/A & N/A \\
        Tau ($\tau$) & $0.95$ (GAE) & $0.005$ (Poly.) & $0.01$ (Soft Update) \\
        Replay Buffer & N/A (On-Policy) & $1 \cdot 10^6$ & $1 \cdot 10^6$ \\
        \bottomrule
    \end{tabular}
\end{table}

Le scelte riportate nella Tabella~\ref{tab:hyperparams} derivano dalle seguenti considerazioni tecniche:

\begin{enumerate}
    \item \textbf{Dimensionamento della Rete}: Per PPO e SAC, è stata scelta una struttura a 3 layer nascosti da 256 neuroni. Test preliminari con reti più profonde (512 o 1024 unità) hanno mostrato un aumento marginale delle prestazioni in simulazione, ma un aumento lineare del tempo di inferenza che avrebbe violato il requisito di frequenza di controllo di 4Hz sul Raspberry Pi.
    \item \textbf{Pianificazione vs Reattività (Horizon)}: Per TD-MPC2, l'orizzonte di pianificazione è stato fissato a $H=6$[cite: 1196]. Dato il passo di controllo di $\Delta t = 0.25s$, questo corrisponde a una previsione di $1.5$ secondi nel futuro. Questo valore è sufficiente per evitare ostacoli dinamici a bassa velocità, mantenendo il costo dell'MPPI entro limiti accettabili. Orizzonti più lunghi ($H > 10$) aumentano esponenzialmente la varianza delle traiettorie immaginate a causa dell'accumulo degli errori del modello dinamico (\emph{compounding errors}).
    \item \textbf{Gestione dell'Entropia}: In SAC e TD-MPC2 (tramite la Policy Prior), l'entropia è massimizzata automaticamente. In PPO, è stato necessario fissare un coefficiente di entropia statico ($0.01$) sufficientemente alto da prevenire il collasso prematuro della policy su azioni deterministiche nelle prime fasi del training, un problema comune in ambienti con reward sparsi.
\end{enumerate}

\section{Summary of Methodology}
\label{sec:methodology_summary}

In questo capitolo è stata definita un'architettura completa per l'apprendimento della navigazione autonoma, che integra simulazione parallela, algoritmi di RL avanzati e vincoli di deployment reale.

Abbiamo stabilito una gerarchia di complessità crescente:
\begin{itemize}
    \item \textbf{Livello 0 (Baseline)}: PPO e SAC senza memoria, che operano sotto l'ipotesi (errata) di Markovianità completa dello stato visivo istantaneo.
    \item \textbf{Livello 1 (Memoria Temporale)}: PPO con Frame Stacking, che mitiga l'assenza di sensori di velocità espliciti ricostruendo la dinamica dalla storia breve.
    \item \textbf{Livello 2 (Memoria di Stato)}: Recurrent PPO, che introduce uno stato nascosto persistente per gestire l'occlusione a lungo termine.
    \item \textbf{Livello 3 (Pianificazione Latente)}: TD-MPC2, che abbandona la policy reattiva pura in favore di un modello predittivo del mondo, teoricamente capace di generalizzazione zero-shot superiore.
\end{itemize}

Questa struttura metodologica non è casuale, ma è disegnata per isolare scientificamente il contributo della "memoria" e della "predizione" nelle prestazioni di navigazione. Nel capitolo successivo, i Risultati Sperimentali (Capitolo 9), verranno quantificati i guadagni prestazionali ottenuti scalando questa gerarchia, dimostrando come la complessità computazionale aggiuntiva si traduca direttamente in un aumento della probabilità di sopravvivenza del rover.